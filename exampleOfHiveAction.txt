<workflow-app name="${jobName}" xmlns="uri:oozie:workflow:0.4">
    <parameters>
        <property>
            <name>jobQueue</name>
        </property>
        <property>
            <name>jobTracker</name>
        </property>
        <property>
            <name>nameNode</name>
        </property>

        <property>
            <name>lookupRawTable</name>
            <description>Name of the HBase table where the raw lookup data should exist when this workflow starts.</description>
        </property>
        <property>
            <name>lookupRawCF</name>
            <value>raw</value>
            <description>Name of the column family that will be used within the lookupRawTable</description>
        </property>
        <property>
            <name>lookupRawQualifer</name>
            <value>pCol</value>
            <description>Name of the qualifier that has the xml data.</description>
        </property>

        <property>
            <name>hbaseZookeeperClientPort</name>
        </property>
        <property>
            <name>hbaseZookeeperQuorum</name>
        </property>

        <property>
            <name>authentication</name>
            <value>kerberos</value>
        </property>
        <property>
            <name>rpcEngine</name>
            <value>org.apache.hadoop.hbase.ipc.SecureRpcEngine</value>
        </property>

        <property>
            <name>mappings</name>
            <description>A comma-separated list of filepaths corresponding to the json mappings that should be used to transform from raw to slim format.</description>
        </property>
        <property>
            <name>slimOutputPath</name>
            <description>Path to where the final slim data will be stored.</description>
        </property>
        <property>
            <name>hiveScriptPath</name>
            <value>${slimOutputPath}/createDatabase.sql</value>
            <description>This is the path to the hive script file that this flow will generate and later call.</description>
        </property>
        <property>
            <name>hiveDatabaseName</name>
            <description>The name of the final hive database this flow creates.</description>
        </property>

        <property>
            <name>hiveSiteLocation</name>
            <description>Path to a hive-site.xml on hdfs.</description>
        </property>
    </parameters>

    <global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>mapreduce.job.queuename</name>
                <value>${jobQueue}</value>
            </property>

            <!-- =========================
                 Start HBase configuration
                 ========================= -->
            <property>
                <name>hbase.zookeeper.quorum</name>
                <value>${hbaseZookeeperQuorum}</value>
            </property>
            <property>
                <name>hbase.rpc.engine</name>
                <value>org.apache.hadoop.hbase.ipc.SecureRpcEngine</value>
            </property>
            <property>
                <name>hbase.security.authentication</name>
                <value>kerberos</value>
            </property>
            <property>
                <name>hbase.zookeeper.property.clientPort</name>
                <value>${hbaseZookeeperClientPort}</value>
            </property>
            <!-- ==========================
                 End HBase configuration
                 ========================== -->
        </configuration>
    </global>

    <credentials>
        <credential name='hbase_cred' type='hbase'>
            <property>
                <name>hbase.zookeeper.quorum</name>
                <value>${hbaseZookeeperQuorum}</value>
            </property>
            <property>
                <name>zookeeper.znode.parent</name>
                <value>/hbase</value>
            </property>
            <property>
                <name>hbase.rpc.engine</name>
                <value>${rpcEngine}</value>
            </property>
            <property>
                <name>hbase.security.authentication</name>
                <value>${authentication}</value>
            </property>
            <property>
                <name>hadoop.security.authentication</name>
                <value>${authentication}</value>
            </property>
            <property>
                <name>hbase.master.kerberos.principal</name>
                <value>hbase/_HOST@PROD-DV.INTERNAL.MACHINES</value>
            </property>
            <property>
                <name>hbase.regionserver.kerberos.principal</name>
                <value>hbase/_HOST@PROD-DV.INTERNAL.MACHINES</value>
            </property>
        </credential>
        <credential name='hive_credentials' type='hcat'>
            <property>
                <name>hcat.metastore.uri</name>
                <value>${hiveMetastoreUris}</value>
            </property>
            <property>
                <name>hcat.metastore.principal</name>
                <value>${hcatMetastorePrincipal}</value>
            </property>
        </credential>
    </credentials>

    <start to="verifyHiveSiteExists"/>

    <!-- Fail early if the hive-site.xml cannot be found -->
    <action name="verifyHiveSiteExists">
        <shell xmlns="uri:oozie:shell-action:0.1">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <exec>verifyFileExists.sh</exec>
            <argument>${hiveSiteLocation}</argument>
            <file>verifyFileExists.sh</file>
        </shell>
        <ok to="run-slim-mapreduce"/>
        <error to="report_failure"/>
    </action>
    <!-- ########################### -->
    <!--   Oozie native map reduce   -->
    <!-- ########################### -->
    <action name="run-slim-mapreduce" cred="hbase_cred">
        <map-reduce>
            <prepare>
                <delete path="${nameNode}${slimOutputPath}"/>
            </prepare>
            <job-xml>hbase-hdfs-mapreduce-job.xml</job-xml>

            <configuration>
                <!-- HBase input configurations -->
                <property>
                    <name>hbase.mapreduce.inputtable</name>
                    <value>${lookupRawTable}</value>
                </property>

                <property>
                    <name>hbase.mapreduce.scan.columns</name>
                    <!-- space separated columnfamily:qualifer -->
                    <!-- Look at the code for details: http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/org.apache.hbase/hbase/0.94.6-cdh4.4.0/org/apache/hadoop/hbase/mapreduce/TableInputFormat.java?av=f -->
                    <value>raw:pCol</value>
                </property>

                <!-- Mapper configuration -->
                <property>
                    <name>mapreduce.map.class</name>
                    <value>com.klarna.datavault.creditlookup.slim.mapreduce.RawCreditLookupToSlimMapper</value>
                </property>
                <property>
                    <name>mapper.multiple.output.facade</name>
                    <value>com.klarna.datavault.creditlookup.slim.mapreduce.multipleoutput.MultipleOutput</value>
                </property>
                <property>
                    <name>mapper.input.table.CFs</name>
                    <value>${lookupRawCF}</value>
                </property>
                <property>
                    <name>mapper.input.table.qualifiers</name>
                    <value>${lookupRawQualifer}</value>
                </property>
                <property>
                    <name>mapper.schema.files</name>
                    <!-- This is present in the datavault-credit-lookup-template jar -->
                    <value>${mappings}</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>${slimOutputPath}</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="generate-hive-script"/>
        <error to="report_failure"/>
    </action>

    <action name="generate-hive-script">
        <java>
            <main-class>com.klarna.datavault.creditlookup.hive.HiveDatabaseCreationScriptGenerator</main-class>
            <arg>${nameNode}</arg>
            <arg>${hiveScriptPath}</arg>
            <arg>${hiveDatabaseName}</arg>
            <arg>${slimOutputPath}</arg>
            <arg>${mappings}</arg>
        </java>
        <ok to="create-hive-tables"/>
        <error to="report_failure"/>
    </action>


    <action name="create-hive-tables" cred="hive_credentials">
        <hive xmlns="uri:oozie:hive-action:0.2">
            <!-- Apparently, the hive-action requires job-tracker and nameNode properties
                 even though they are defined in the global section. -->
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <job-xml>${hiveSiteLocation}</job-xml>
            <script>${hiveScriptPath}</script>
        </hive>
        <ok to="end"/>
        <error to="report_failure"/>
    </action>

    <!-- =========================================== -->
    <!-- Start of standard failure reporting snippet -->
    <!-- =========================================== -->

    <action name="report_failure">
        <email xmlns="uri:oozie:email-action:0.1">
            <to>${oozie_email_to_address}</to>
            <subject>Oozie workflow "${jobName}" failed</subject>
            <body>
                Job name: ${jobName}
                Oozie workflow id: ${wf:id()}
                Last error node: ${wf:lastErrorNode()}
                Error message: ${wf:errorMessage(wf:lastErrorNode())}
                Name node: ${nameNode}
                Job tracker: ${jobTracker}
                Direct link to hue console: ${hueUrl}/oozie/list_oozie_workflow/${wf:id()}//
            </body>
        </email>
        <ok to="kill"/>
        <error to="kill"/>
    </action>

    <kill name="kill">
        <message>Oozie job "${jobName}" ${hueUrl}/oozie/list_oozie_workflow/${wf:id()}// (jobtracker [${jobTracker}], namenode [${nameNode}]) failed at (wf:lastErrorNode()) with error message ${wf:errorMessage(wf:lastErrorNode())}</message>
    </kill>

    <!-- =========================================== -->
    <!-- End of standard failure reporting snippet -->
    <!-- =========================================== -->

    <end name="end"/>
</workflow-app> 
